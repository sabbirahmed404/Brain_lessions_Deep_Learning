{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c3c5bc3",
      "metadata": {
        "id": "7c3c5bc3"
      },
      "source": [
        "# Multi-Modal Brain Tumor Detection System\n",
        "## Integrating MRI and PET Image Analysis with YOLOv8\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. DICOM to JPG conversion for PET images\n",
        "2. Automatic modality detection (MRI vs PET)\n",
        "3. Dual-model training and inference\n",
        "4. Ensemble prediction for paired images\n",
        "5. Performance comparison and visualization\n",
        "\n",
        "**Author:** [Your Name]  \n",
        "**Date:** 2025-10-09"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0dIHEC6TVjc",
        "outputId": "6dd11296-c528-4748-8ce4-a53ce023535d"
      },
      "id": "s0dIHEC6TVjc",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a675c148",
      "metadata": {
        "id": "a675c148"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "0f24edda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f24edda",
        "outputId": "db75c0d4-5ff8-4c46-92ea-4a76d5ddc1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run once)\n",
        "# !pip install ultralytics opencv-python pydicom numpy pandas matplotlib seaborn tqdm\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('ggplot')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65562b44",
      "metadata": {
        "id": "65562b44"
      },
      "source": [
        "## 2. DICOM Conversion Utilities\n",
        "\n",
        "Convert medical DICOM files to YOLO-compatible JPG format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "f16b6e4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f16b6e4d",
        "outputId": "b9af824a-6cde-4ff6-dc4c-c55113484ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DICOM Converter ready\n"
          ]
        }
      ],
      "source": [
        "import pydicom\n",
        "\n",
        "class DICOMConverter:\n",
        "    def __init__(self, normalize=True, resize=640):\n",
        "        self.normalize = normalize\n",
        "        self.resize = resize\n",
        "\n",
        "    def read_dicom(self, dicom_path):\n",
        "        try:\n",
        "            ds = pydicom.dcmread(dicom_path)\n",
        "            img = ds.pixel_array\n",
        "            if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
        "                img = img * ds.RescaleSlope + ds.RescaleIntercept\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {dicom_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def normalize_image(self, img):\n",
        "        if img is None or img.size == 0:\n",
        "            return None\n",
        "        img = img.astype(np.float32)\n",
        "        img_min, img_max = img.min(), img.max()\n",
        "        if img_max - img_min > 0:\n",
        "            img = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)\n",
        "        else:\n",
        "            img = np.zeros_like(img, dtype=np.uint8)\n",
        "        return img\n",
        "\n",
        "    def enhance_contrast(self, img, clip_limit=2.0):\n",
        "        if img is None:\n",
        "            return None\n",
        "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
        "        return clahe.apply(img)\n",
        "\n",
        "    def resize_image(self, img, size=640):\n",
        "        if img is None:\n",
        "            return None\n",
        "        h, w = img.shape[:2]\n",
        "        if h > w:\n",
        "            new_h, new_w = size, int(w * (size / h))\n",
        "        else:\n",
        "            new_w, new_h = size, int(h * (size / w))\n",
        "        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "        delta_w, delta_h = size - new_w, size - new_h\n",
        "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "        return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "    def convert_dicom_to_jpg(self, dicom_path, output_path, enhance=True):\n",
        "        img = self.read_dicom(dicom_path)\n",
        "        if img is None:\n",
        "            return False\n",
        "        if self.normalize:\n",
        "            img = self.normalize_image(img)\n",
        "        if enhance:\n",
        "            img = self.enhance_contrast(img)\n",
        "        if self.resize:\n",
        "            img = self.resize_image(img, self.resize)\n",
        "        try:\n",
        "            cv2.imwrite(output_path, img)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {output_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def batch_convert(self, input_dir, output_dir, modality=\"MRI\"):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        dicom_files = list(Path(input_dir).glob(\"*.dcm\"))\n",
        "        print(f\"Found {len(dicom_files)} DICOM files\")\n",
        "        success = 0\n",
        "        for i, dicom_file in enumerate(tqdm(dicom_files, desc=f\"Converting {modality}\")):\n",
        "            output_name = f\"{modality.lower()}_{i+1:04d}.jpg\"\n",
        "            output_path = os.path.join(output_dir, output_name)\n",
        "            if self.convert_dicom_to_jpg(str(dicom_file), output_path):\n",
        "                success += 1\n",
        "        print(f\"Converted {success}/{len(dicom_files)} files\")\n",
        "        return success\n",
        "\n",
        "print(\"✅ DICOM Converter ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c74f5ff",
      "metadata": {
        "id": "7c74f5ff"
      },
      "source": [
        "## 3. Automatic Modality Detection\n",
        "\n",
        "Classify images as MRI or PET based on visual features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "2b81466c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b81466c",
        "outputId": "828d8ce8-005c-4b8e-dc09-1bfb8c78db1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modality Classifier ready\n"
          ]
        }
      ],
      "source": [
        "class ModalityClassifier:\n",
        "    def detect_modality(self, image_path):\n",
        "        # Check path first - if 'pet' in path, it's PET\n",
        "        path_lower = image_path.lower()\n",
        "        if 'pet' in path_lower:\n",
        "            return \"PET\"\n",
        "        elif 'mri' in path_lower:\n",
        "            return \"MRI\"\n",
        "\n",
        "        # Fallback to image analysis\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            return \"MRI\"\n",
        "\n",
        "        # Feature extraction\n",
        "        mean_intensity = np.mean(img)\n",
        "        std_intensity = np.std(img)\n",
        "        contrast = img.max() - img.min()\n",
        "\n",
        "        # Gradient analysis\n",
        "        grad_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        grad_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        edge_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "        edge_density = np.mean(edge_magnitude)\n",
        "\n",
        "        # Histogram analysis\n",
        "        hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
        "        hist_peak = np.argmax(hist)\n",
        "\n",
        "        # Adjusted thresholds for PET vs MRI\n",
        "        if edge_density < 20 and hist_peak > 80:\n",
        "            return \"PET\"\n",
        "        elif edge_density > 25 and contrast > 120:\n",
        "            return \"MRI\"\n",
        "        else:\n",
        "            upper_quartile = np.percentile(img, 75)\n",
        "            if upper_quartile > 140 and std_intensity < 60:\n",
        "                return \"PET\"\n",
        "            return \"MRI\"\n",
        "\n",
        "print(\"✅ Modality Classifier ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a4eadc0",
      "metadata": {
        "id": "6a4eadc0"
      },
      "source": [
        "## 4. Multi-Modal Detection System\n",
        "\n",
        "Main detector class that integrates MRI and PET models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "32ceaad1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ceaad1",
        "outputId": "13922eda-126f-49ab-c866-629cee1987b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Multi-Modal Detector class ready\n"
          ]
        }
      ],
      "source": [
        "class MultiModalBrainTumorDetector:\n",
        "    def __init__(self, mri_model_path, pet_model_path=None, auto_detect_modality=True, confidence_threshold=0.5):\n",
        "        self.auto_detect = auto_detect_modality\n",
        "        self.conf_threshold = confidence_threshold\n",
        "        self.modality_classifier = ModalityClassifier()\n",
        "\n",
        "        # Load models\n",
        "        print(\"Loading models...\")\n",
        "        self.mri_model = YOLO(mri_model_path) if os.path.exists(mri_model_path) else None\n",
        "\n",
        "        if pet_model_path and os.path.exists(pet_model_path):\n",
        "            self.pet_model = YOLO(pet_model_path)\n",
        "            print(\"✅ Both MRI and PET models loaded\")\n",
        "        else:\n",
        "            self.pet_model = None\n",
        "            print(\"⚠️  Only MRI model loaded\")\n",
        "\n",
        "        self.stats = {'mri_detections': 0, 'pet_detections': 0, 'total_predictions': 0}\n",
        "\n",
        "    def predict(self, image_path, modality=None, save_results=False, output_dir=\"predictions\"):\n",
        "        if not os.path.exists(image_path):\n",
        "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "        # Detect modality\n",
        "        if modality is None and self.auto_detect:\n",
        "            modality = self.modality_classifier.detect_modality(image_path)\n",
        "            print(f\"Detected modality: {modality}\")\n",
        "        elif modality is None:\n",
        "            modality = \"MRI\"\n",
        "\n",
        "        # Select model\n",
        "        if modality.upper() == \"PET\" and self.pet_model is not None:\n",
        "            model = self.pet_model\n",
        "            self.stats['pet_detections'] += 1\n",
        "        else:\n",
        "            model = self.mri_model\n",
        "            self.stats['mri_detections'] += 1\n",
        "\n",
        "        if model is None:\n",
        "            raise ValueError(\"No model available\")\n",
        "\n",
        "        # Run inference\n",
        "        results = model.predict(source=image_path, conf=self.conf_threshold, save=save_results,\n",
        "                               project=output_dir, name=f\"{modality.lower()}_predictions\")\n",
        "\n",
        "        self.stats['total_predictions'] += 1\n",
        "\n",
        "        # Parse results\n",
        "        return self._parse_results(results[0], modality, image_path)\n",
        "\n",
        "    def _parse_results(self, result, modality, image_path):\n",
        "        boxes = result.boxes\n",
        "        detections = []\n",
        "        for box in boxes:\n",
        "            detections.append({\n",
        "                'class': int(box.cls[0]),\n",
        "                'class_name': result.names[int(box.cls[0])],\n",
        "                'confidence': float(box.conf[0]),\n",
        "                'bbox': box.xyxy[0].tolist()\n",
        "            })\n",
        "        return {\n",
        "            'image_path': image_path,\n",
        "            'modality': modality,\n",
        "            'num_detections': len(detections),\n",
        "            'detections': detections,\n",
        "            'has_tumor': len(detections) > 0\n",
        "        }\n",
        "\n",
        "    def visualize_results(self, image_path, predictions, save_path=None):\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            return\n",
        "\n",
        "        for det in predictions.get('detections', []):\n",
        "            bbox = det['bbox']\n",
        "            conf = det['confidence']\n",
        "            label = f\"{det['class_name']}: {conf:.2f}\"\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        modality = predictions.get('modality', 'Unknown')\n",
        "        cv2.putText(img, f\"Modality: {modality}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        if save_path:\n",
        "            cv2.imwrite(save_path, img)\n",
        "            print(f\"Saved to {save_path}\")\n",
        "        else:\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.imshow(img_rgb)\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"{modality} - {'TUMOR DETECTED' if predictions['has_tumor'] else 'NO TUMOR'}\")\n",
        "            plt.show()\n",
        "\n",
        "    def get_statistics(self):\n",
        "        return self.stats.copy()\n",
        "\n",
        "print(\"✅ Multi-Modal Detector class ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe6cc08e",
      "metadata": {
        "id": "fe6cc08e"
      },
      "source": [
        "## 5. Convert PET DICOM Files\n",
        "\n",
        "Convert your PET DICOM files to JPG format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "a6760a68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6760a68",
        "outputId": "00cf3680-dfc4-4d6f-9b05-a84cb01cd5e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 82 DICOM files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PET: 100%|██████████| 82/82 [00:00<00:00, 133.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 82/82 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert PET DICOM files\n",
        "converter = DICOMConverter(normalize=True, resize=640)\n",
        "\n",
        "pet_dicom_dir = \"/content/Brain_Tumor_Detection_Using_YOLO/Pet+Mri/data/BrainTumorPET\"\n",
        "pet_output_dir = \"/content/Brain_Tumor_Detection_Using_YOLO/pet_converted_images\"\n",
        "\n",
        "if os.path.exists(pet_dicom_dir):\n",
        "    converter.batch_convert(pet_dicom_dir, pet_output_dir, modality=\"PET\")\n",
        "else:\n",
        "    print(f\"⚠️  Directory not found: {pet_dicom_dir}\")\n",
        "    print(\"Update the path to your PET DICOM directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a717cb",
      "metadata": {
        "id": "15a717cb"
      },
      "source": [
        "## 6. Prepare YOLO Dataset Structure\n",
        "\n",
        "Create train/val/test splits for PET images.\n",
        "\n",
        "⚠️ **Important:** After running this cell, you need to annotate images using:\n",
        "- LabelImg: `pip install labelImg`\n",
        "- Roboflow: https://roboflow.com\n",
        "- CVAT: https://www.cvat.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "569c6ea6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "569c6ea6",
        "outputId": "cd8caae8-ae23-40eb-95d9-c4538d667d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run cell 5 first to convert DICOM files\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "def prepare_yolo_dataset(pet_images_dir, output_base_dir=\"pet_tumor_dataset\", train_ratio=0.7, val_ratio=0.2):\n",
        "    # Create directory structure\n",
        "    dirs = {\n",
        "        'train_images': os.path.join(output_base_dir, 'images', 'train'),\n",
        "        'val_images': os.path.join(output_base_dir, 'images', 'val'),\n",
        "        'test_images': os.path.join(output_base_dir, 'images', 'test'),\n",
        "        'train_labels': os.path.join(output_base_dir, 'labels', 'train'),\n",
        "        'val_labels': os.path.join(output_base_dir, 'labels', 'val'),\n",
        "        'test_labels': os.path.join(output_base_dir, 'labels', 'test'),\n",
        "    }\n",
        "\n",
        "    for dir_path in dirs.values():\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "    # Get and shuffle images\n",
        "    images = list(Path(pet_images_dir).glob(\"*.jpg\"))\n",
        "    total = len(images)\n",
        "    np.random.shuffle(images)\n",
        "\n",
        "    # Split\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end = train_end + int(total * val_ratio)\n",
        "\n",
        "    train_images = images[:train_end]\n",
        "    val_images = images[train_end:val_end]\n",
        "    test_images = images[val_end:]\n",
        "\n",
        "    # Copy images\n",
        "    print(\"Organizing images...\")\n",
        "    for img in tqdm(train_images, desc=\"Train\"):\n",
        "        shutil.copy(str(img), dirs['train_images'])\n",
        "    for img in tqdm(val_images, desc=\"Val\"):\n",
        "        shutil.copy(str(img), dirs['val_images'])\n",
        "    for img in tqdm(test_images, desc=\"Test\"):\n",
        "        shutil.copy(str(img), dirs['test_images'])\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_images)} | Val: {len(val_images)} | Test: {len(test_images)}\")\n",
        "\n",
        "    # Create config\n",
        "    config_content = f\"\"\"path: {output_base_dir}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "names: [Tumor]\n",
        "\"\"\"\n",
        "    config_path = os.path.join(output_base_dir, 'config.yaml')\n",
        "    with open(config_path, 'w') as f:\n",
        "        f.write(config_content)\n",
        "    print(f\"Config saved: {config_path}\")\n",
        "\n",
        "    print(\"\\n⚠️  NEXT STEP: Annotate images in:\")\n",
        "    print(f\"   {dirs['train_images']}\")\n",
        "    print(f\"   Place labels in: {dirs['train_labels']}\")\n",
        "\n",
        "# Run preparation\n",
        "if os.path.exists(\"pet_converted_images\"):\n",
        "    prepare_yolo_dataset(\"pet_converted_images\", \"pet_tumor_dataset\")\n",
        "else:\n",
        "    print(\"Run cell 5 first to convert DICOM files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "validation_section",
      "metadata": {
        "id": "validation_section"
      },
      "source": [
        "## 6.5 Validate Dataset - CRITICAL!\n",
        "\n",
        "⚠️ **STOP! Check if your dataset has labels before training!**\n",
        "\n",
        "This will verify:\n",
        "- ✅ Images exist\n",
        "- ✅ Labels exist (bounding box annotations)\n",
        "- ✅ Annotation coverage\n",
        "\n",
        "**Without labels, training will fail or do nothing!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "285493c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "285493c1",
        "outputId": "0f655518-44d5-4b09-e060-9844d9aebf11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test labels generated!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "def quick_fix_labels():\n",
        "    base = Path(\"/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset\")\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        img_dir = base / 'images' / split\n",
        "        label_dir = base / 'labels' / split\n",
        "        label_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        for img in img_dir.glob(\"*.jpg\"):\n",
        "            label_file = label_dir / f\"{img.stem}.txt\"\n",
        "            with open(label_file, 'w') as f:\n",
        "                # Random box: class x y w h\n",
        "                f.write(f\"0 {random.uniform(0.4,0.6):.4f} {random.uniform(0.4,0.6):.4f} 0.2 0.2\\n\")\n",
        "    print(\"✅ Test labels generated!\")\n",
        "\n",
        "quick_fix_labels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "validation_code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "validation_code",
        "outputId": "edcb77a8-6c8b-40c6-c617-6378c71a5507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATASET VALIDATION\n",
            "======================================================================\n",
            "\n",
            "TRAIN:\n",
            "  Images: 82\n",
            "  Labels: 82\n",
            "  ✅ Fully labeled\n",
            "\n",
            "VAL:\n",
            "  Images: 63\n",
            "  Labels: 63\n",
            "  ✅ Fully labeled\n",
            "\n",
            "======================================================================\n",
            "✅ VALIDATION PASSED\n",
            "   145/145 images labeled (100.0%)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "def validate_dataset(config_path=\"/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/config.yaml\"):\n",
        "    \"\"\"Validate dataset has labels before training\"\"\"\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"❌ Config not found: {config_path}\")\n",
        "        return False\n",
        "\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    base_path = Path(config.get('path', '/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset'))\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"DATASET VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total_images = 0\n",
        "    total_labels = 0\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        img_dir = base_path / 'images' / split\n",
        "        label_dir = base_path / 'labels' / split\n",
        "\n",
        "        if img_dir.exists():\n",
        "            images = list(img_dir.glob(\"*.jpg\"))\n",
        "            labels = list(label_dir.glob(\"*.txt\")) if label_dir.exists() else []\n",
        "\n",
        "            total_images += len(images)\n",
        "            total_labels += len(labels)\n",
        "\n",
        "            print(f\"\\n{split.upper()}:\")\n",
        "            print(f\"  Images: {len(images)}\")\n",
        "            print(f\"  Labels: {len(labels)}\")\n",
        "\n",
        "            if len(labels) == 0:\n",
        "                print(f\"  ❌ NO LABELS!\")\n",
        "            elif len(labels) < len(images):\n",
        "                print(f\"  ⚠️  {len(images) - len(labels)} images missing labels\")\n",
        "            else:\n",
        "                print(f\"  ✅ Fully labeled\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    if total_labels == 0:\n",
        "        print(\"❌ VALIDATION FAILED - NO LABELS FOUND!\")\n",
        "        print(\"\\nYou have images but NO annotations!\")\n",
        "        print(\"\\n🔧 SOLUTIONS:\")\n",
        "        print(\"\\n1. ANNOTATE MANUALLY (Required for thesis):\")\n",
        "        print(\"   Run in terminal: pip install labelImg && labelImg\")\n",
        "        print(\"   - Open Dir: pet_tumor_dataset/images/train\")\n",
        "        print(\"   - Save Dir: pet_tumor_dataset/labels/train\")\n",
        "        print(\"   - Format: YOLO\")\n",
        "        print(\"   - Draw boxes (W), save (Ctrl+S), next (D)\")\n",
        "        print(\"\\n2. GENERATE DUMMY LABELS (Testing only):\")\n",
        "        print(\"   Run next cell to generate fake labels\")\n",
        "        print(\"   ⚠️  WARNING: Not for real training!\")\n",
        "        print(\"\\n3. READ GUIDE:\")\n",
        "        print(\"   See ANNOTATION_GUIDE.md for complete instructions\")\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    else:\n",
        "        coverage = (total_labels / total_images * 100) if total_images > 0 else 0\n",
        "        print(f\"✅ VALIDATION PASSED\")\n",
        "        print(f\"   {total_labels}/{total_images} images labeled ({coverage:.1f}%)\")\n",
        "        print(\"=\"*70)\n",
        "        return True\n",
        "\n",
        "# Run validation\n",
        "dataset_is_valid = validate_dataset()\n",
        "\n",
        "if not dataset_is_valid:\n",
        "    print(\"\\n⚠️  Cannot proceed to training without labels!\")\n",
        "    print(\"Please annotate or run next cell to generate dummy labels.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dummy_labels_section",
      "metadata": {
        "id": "dummy_labels_section"
      },
      "source": [
        "## 6.6 Generate Dummy Labels (TESTING ONLY)\n",
        "\n",
        "⚠️ **WARNING: This generates FAKE random bounding boxes!**\n",
        "\n",
        "**Use ONLY to test if the pipeline works.**\n",
        "\n",
        "**DO NOT use for your thesis!** Real manual annotation is required for valid research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "dummy_labels_code",
      "metadata": {
        "id": "dummy_labels_code"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_dummy_labels():\n",
        "    \"\"\"Generate fake labels for testing - NOT FOR REAL USE!\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"⚠️  DUMMY LABEL GENERATOR - FAKE DATA WARNING!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nThis will create RANDOM bounding boxes.\")\n",
        "    print(\"These are NOT real tumor annotations!\")\n",
        "    print(\"\\nUse ONLY to test if training works.\")\n",
        "    print(\"For your thesis, you MUST annotate manually!\\n\")\n",
        "\n",
        "    response = input(\"Generate dummy labels? (yes/no): \")\n",
        "    if response.lower() not in ['yes', 'y']:\n",
        "        print(\"Cancelled.\")\n",
        "        return\n",
        "\n",
        "    base_path = Path(\"/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset\")\n",
        "    total = 0\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        img_dir = base_path / 'images' / split\n",
        "        label_dir = base_path / 'labels' / split\n",
        "\n",
        "        if not img_dir.exists():\n",
        "            continue\n",
        "\n",
        "        label_dir.mkdir(parents=True, exist_ok=True)\n",
        "        images = list(img_dir.glob(\"*.jpg\"))\n",
        "\n",
        "        print(f\"Generating {len(images)} labels for {split}...\")\n",
        "\n",
        "        for img_path in images:\n",
        "            label_path = label_dir / (img_path.stem + '.txt')\n",
        "            num_boxes = random.randint(1, 2)\n",
        "\n",
        "            with open(label_path, 'w') as f:\n",
        "                for _ in range(num_boxes):\n",
        "                    # YOLO format: class x_center y_center width height\n",
        "                    class_id = 0\n",
        "                    x = random.uniform(0.3, 0.7)\n",
        "                    y = random.uniform(0.3, 0.7)\n",
        "                    w = random.uniform(0.15, 0.35)\n",
        "                    h = random.uniform(0.15, 0.35)\n",
        "                    f.write(f\"{class_id} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
        "            total += 1\n",
        "\n",
        "        print(f\"✅ {len(images)} labels for {split}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"✅ Generated {total} dummy label files\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n⚠️  REMEMBER: These are FAKE labels!\")\n",
        "    print(\"For real research, delete these and annotate manually!\")\n",
        "    print(\"\\nNow run the validation cell again to verify.\")\n",
        "\n",
        "# Uncomment to generate dummy labels (TESTING ONLY!)\n",
        "# generate_dummy_labels()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKHq338zTPYO"
      },
      "source": [
        "## 6.5 Validate Dataset - CRITICAL!\n",
        "\n",
        "⚠️ **STOP! Check if your dataset has labels before training!**\n",
        "\n",
        "This will verify:\n",
        "- ✅ Images exist\n",
        "- ✅ Labels exist (bounding box annotations)\n",
        "- ✅ Annotation coverage\n",
        "\n",
        "**Without labels, training will fail or do nothing!**"
      ],
      "id": "yKHq338zTPYO"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htVG1hBLTPYO",
        "outputId": "f5f9c397-5249-4b79-eeae-b4b59edc953b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATASET VALIDATION\n",
            "======================================================================\n",
            "\n",
            "TRAIN:\n",
            "  Images: 82\n",
            "  Labels: 82\n",
            "  ✅ Fully labeled\n",
            "\n",
            "VAL:\n",
            "  Images: 63\n",
            "  Labels: 63\n",
            "  ✅ Fully labeled\n",
            "\n",
            "======================================================================\n",
            "✅ VALIDATION PASSED\n",
            "   145/145 images labeled (100.0%)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def validate_dataset(config_path=\"/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/config.yaml\"):\n",
        "    \"\"\"Validate dataset has labels before training\"\"\"\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"❌ Config not found: {config_path}\")\n",
        "        return False\n",
        "\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    base_path = Path(config.get('path', 'pet_tumor_dataset'))\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"DATASET VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total_images = 0\n",
        "    total_labels = 0\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        img_dir = base_path / 'images' / split\n",
        "        label_dir = base_path / 'labels' / split\n",
        "\n",
        "        if img_dir.exists():\n",
        "            images = list(img_dir.glob(\"*.jpg\"))\n",
        "            labels = list(label_dir.glob(\"*.txt\")) if label_dir.exists() else []\n",
        "\n",
        "            total_images += len(images)\n",
        "            total_labels += len(labels)\n",
        "\n",
        "            print(f\"\\n{split.upper()}:\")\n",
        "            print(f\"  Images: {len(images)}\")\n",
        "            print(f\"  Labels: {len(labels)}\")\n",
        "\n",
        "            if len(labels) == 0:\n",
        "                print(f\"  ❌ NO LABELS!\")\n",
        "            elif len(labels) < len(images):\n",
        "                print(f\"  ⚠️  {len(images) - len(labels)} images missing labels\")\n",
        "            else:\n",
        "                print(f\"  ✅ Fully labeled\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    if total_labels == 0:\n",
        "        print(\"❌ VALIDATION FAILED - NO LABELS FOUND!\")\n",
        "        print(\"\\nYou have images but NO annotations!\")\n",
        "        print(\"\\n🔧 SOLUTIONS:\")\n",
        "        print(\"\\n1. ANNOTATE MANUALLY (Required for thesis):\")\n",
        "        print(\"   Run in terminal: pip install labelImg && labelImg\")\n",
        "        print(\"   - Open Dir: pet_tumor_dataset/images/train\")\n",
        "        print(\"   - Save Dir: pet_tumor_dataset/labels/train\")\n",
        "        print(\"   - Format: YOLO\")\n",
        "        print(\"   - Draw boxes (W), save (Ctrl+S), next (D)\")\n",
        "        print(\"\\n2. GENERATE DUMMY LABELS (Testing only):\")\n",
        "        print(\"   Run next cell to generate fake labels\")\n",
        "        print(\"   ⚠️  WARNING: Not for real training!\")\n",
        "        print(\"\\n3. READ GUIDE:\")\n",
        "        print(\"   See ANNOTATION_GUIDE.md for complete instructions\")\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "    else:\n",
        "        coverage = (total_labels / total_images * 100) if total_images > 0 else 0\n",
        "        print(f\"✅ VALIDATION PASSED\")\n",
        "        print(f\"   {total_labels}/{total_images} images labeled ({coverage:.1f}%)\")\n",
        "        print(\"=\"*70)\n",
        "        return True\n",
        "\n",
        "# Run validation\n",
        "dataset_is_valid = validate_dataset()\n",
        "\n",
        "if not dataset_is_valid:\n",
        "    print(\"\\n⚠️  Cannot proceed to training without labels!\")\n",
        "    print(\"Please annotate or run next cell to generate dummy labels.\")"
      ],
      "id": "htVG1hBLTPYO"
    },
    {
      "cell_type": "markdown",
      "id": "8052b2ee",
      "metadata": {
        "id": "8052b2ee"
      },
      "source": [
        "## 7. Train PET Model\n",
        "\n",
        "Train YOLOv8 on annotated PET images.\n",
        "\n",
        "⚠️ **Prerequisites:**\n",
        "1. Images must be annotated\n",
        "2. Label files (.txt) must be in labels/train, labels/val directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "0aadf65a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0aadf65a",
        "outputId": "55a55e97-b453-4657-8bf7-f9710530be2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 82 label files\n",
            "\n",
            "🚀 Training PET model...\n",
            "Epochs: 100 | Batch: 32\n",
            "Ultralytics 8.3.208 🚀 Python-3.12.11 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=disk, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/config.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8_pet_detection2, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=pet_tumor_project, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/pet_tumor_project/yolov8_pet_detection2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 14.3MB/s 0.1s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 529.5±417.0 MB/s, size: 43.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/labels/train... 82 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 82/82 704.6it/s 0.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/labels/train.cache\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB Disk): 100% ━━━━━━━━━━━━ 82/82 28.5Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 732.5±507.0 MB/s, size: 45.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/labels/val... 63 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 63/63 1.4Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/labels/val.cache\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB Disk): 100% ━━━━━━━━━━━━ 63/63 20.6Kit/s 0.0s\n",
            "Plotting labels to /content/pet_tumor_project/yolov8_pet_detection2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/pet_tumor_project/yolov8_pet_detection2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100         0G      2.517      4.437      2.485         26        640: 100% ━━━━━━━━━━━━ 3/3 0.0it/s 1:60\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 0.0it/s 26.4s\n",
            "                   all         63         63     0.0027       0.81    0.00249    0.00104\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K: 0% ──────────── 0/3  12.7s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1319788842.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     results = model.train(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_yaml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for cases of training and validating while training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for cases of training and validating while training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m_predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/head.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Training path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "data_yaml = \"/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_dataset/config.yaml\"\n",
        "epochs = 100\n",
        "batch = 32\n",
        "\n",
        "# Check if dataset config exists\n",
        "if not os.path.exists(data_yaml):\n",
        "    print(f\"❌ Dataset config not found: {data_yaml}\")\n",
        "    print(\"Run previous cells to prepare dataset\")\n",
        "else:\n",
        "    # Load YAML config\n",
        "    with open(data_yaml, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    # Check for label files\n",
        "    labels_dir = Path(config['path']) / 'labels' / 'train'\n",
        "    label_count = len(list(labels_dir.glob(\"*.txt\"))) if labels_dir.exists() else 0\n",
        "\n",
        "    print(f\"Found {label_count} label files\")\n",
        "    if label_count == 0:\n",
        "        print(\"⚠️  No labels found! Please annotate images first.\")\n",
        "        response = input(\"Continue anyway? (y/n): \")\n",
        "        if response.lower() != 'y':\n",
        "            exit()\n",
        "\n",
        "    # Initialize YOLO model\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "    print(f\"\\n🚀 Training PET model...\")\n",
        "    print(f\"Epochs: {epochs} | Batch: {batch}\")\n",
        "\n",
        "    # Train the model\n",
        "    results = model.train(\n",
        "        data=data_yaml,\n",
        "        epochs=epochs,\n",
        "        patience=10,\n",
        "        batch=batch,\n",
        "        imgsz=640,\n",
        "        device=0 if torch.cuda.is_available() else 'cpu',\n",
        "        optimizer=\"AdamW\",\n",
        "        lr0=0.01,\n",
        "        lrf=0.01,\n",
        "        warmup_epochs=3,\n",
        "        save=True,\n",
        "        cache=\"disk\",\n",
        "        amp=True,\n",
        "        workers=0,\n",
        "        project=\"pet_tumor_project\",\n",
        "        name=\"yolov8_pet_detection\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ Training complete!\")\n",
        "    print(\"Model saved to: pet_tumor_project/yolov8_pet_detection/weights/best.pt\")\n",
        "# Uncomment to train (after annotation)\n",
        "# train_results = train_pet_model(epochs=100, batch=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8cdd19",
      "metadata": {
        "id": "dc8cdd19"
      },
      "source": [
        "## 8. Initialize Multi-Modal Detector\n",
        "\n",
        "Load your trained models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dWU7rCOGXyIV"
      },
      "id": "dWU7rCOGXyIV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e97067f",
      "metadata": {
        "id": "9e97067f"
      },
      "outputs": [],
      "source": [
        "# Model paths\n",
        "mri_model_path = \"/content/Brain_Tumor_Detection_Using_YOLO/brain_tumor_project/yolov8_object_detection4/weights/best.pt\"\n",
        "pet_model_path = \"/content/Brain_Tumor_Detection_Using_YOLO/pet_tumor_project/yolov8_pet_detection/weights/best.pt\"\n",
        "\n",
        "# Check what's available\n",
        "print(\"Model Status:\")\n",
        "print(f\"✅ MRI Model\" if os.path.exists(mri_model_path) else f\"❌ MRI Model: {mri_model_path}\")\n",
        "print(f\"✅ PET Model\" if os.path.exists(pet_model_path) else f\"⏳ PET Model: Not trained yet\")\n",
        "\n",
        "# Initialize detector\n",
        "detector = MultiModalBrainTumorDetector(\n",
        "    mri_model_path=mri_model_path,\n",
        "    pet_model_path=pet_model_path if os.path.exists(pet_model_path) else None,\n",
        "    auto_detect_modality=True,\n",
        "    confidence_threshold=0.5\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Detector initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3d82f7",
      "metadata": {
        "id": "4b3d82f7"
      },
      "source": [
        "## 9. Test on Single Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b4db07",
      "metadata": {
        "id": "69b4db07"
      },
      "outputs": [],
      "source": [
        "# Test on a single image\n",
        "test_image = \"pet_tumor_dataset/images/test/pet_0001.jpg\"  # Change this path\n",
        "\n",
        "\n",
        "if os.path.exists(test_image):\n",
        "    result = detector.predict(test_image, save_results=False)\n",
        "\n",
        "    print(f\"\\nResults for: {os.path.basename(test_image)}\")\n",
        "    print(f\"Modality: {result['modality']}\")\n",
        "    print(f\"Tumor Detected: {result['has_tumor']}\")\n",
        "    print(f\"Detections: {result['num_detections']}\")\n",
        "\n",
        "    if result['detections']:\n",
        "        for i, det in enumerate(result['detections'], 1):\n",
        "            print(f\"  {i}. {det['class_name']}: {det['confidence']:.3f}\")\n",
        "\n",
        "    # Visualize\n",
        "    detector.visualize_results(test_image, result)\n",
        "else:\n",
        "    print(f\"Image not found: {test_image}\")\n",
        "    print(\"Update test_image path to a valid image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82573c6b",
      "metadata": {
        "id": "82573c6b"
      },
      "source": [
        "## 10. Batch Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112ce6d8",
      "metadata": {
        "id": "112ce6d8"
      },
      "outputs": [],
      "source": [
        "# Test on multiple images\n",
        "test_dir = \"brain-tumor-dataset-splitted/images/test\"\n",
        "\n",
        "if os.path.exists(test_dir):\n",
        "    test_images = list(Path(test_dir).glob(\"*.jpg\"))[:10]  # First 10 images\n",
        "\n",
        "    results = []\n",
        "    for img_path in tqdm(test_images, desc=\"Processing\"):\n",
        "        result = detector.predict(str(img_path), save_results=False)\n",
        "        results.append(result)\n",
        "\n",
        "    # Create summary\n",
        "    df = pd.DataFrame([{\n",
        "        'Image': os.path.basename(r['image_path']),\n",
        "        'Modality': r['modality'],\n",
        "        'Tumor': 'YES' if r['has_tumor'] else 'NO',\n",
        "        'Confidence': max([d['confidence'] for d in r['detections']], default=0)\n",
        "    } for r in results])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BATCH RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    # Statistics\n",
        "    print(f\"\\nTumors detected: {sum(1 for r in results if r['has_tumor'])}/{len(results)}\")\n",
        "    print(f\"MRI images: {sum(1 for r in results if r['modality']=='MRI')}\")\n",
        "    print(f\"PET images: {sum(1 for r in results if r['modality']=='PET')}\")\n",
        "else:\n",
        "    print(f\"Test directory not found: {test_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917e314d",
      "metadata": {
        "id": "917e314d"
      },
      "source": [
        "## 11. Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a576dd20",
      "metadata": {
        "id": "a576dd20"
      },
      "outputs": [],
      "source": [
        "# Visualize multiple results\n",
        "if 'results' in locals() and len(results) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, result in enumerate(results[:6]):  # First 6 results\n",
        "        img = cv2.imread(result['image_path'])\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Draw boxes\n",
        "        for det in result['detections']:\n",
        "            bbox = det['bbox']\n",
        "            x1, y1, x2, y2 = map(int, bbox)\n",
        "            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
        "\n",
        "        axes[i].imshow(img_rgb)\n",
        "        axes[i].axis('off')\n",
        "        title = f\"{result['modality']} - {'TUMOR' if result['has_tumor'] else 'CLEAR'}\"\n",
        "        axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Run batch testing first (Cell 10)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abdcb8c9",
      "metadata": {
        "id": "abdcb8c9"
      },
      "source": [
        "## 12. Detection Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b852c0d",
      "metadata": {
        "id": "4b852c0d"
      },
      "outputs": [],
      "source": [
        "stats = detector.get_statistics()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DETECTION STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total predictions: {stats['total_predictions']}\")\n",
        "print(f\"MRI detections: {stats['mri_detections']}\")\n",
        "print(f\"PET detections: {stats['pet_detections']}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Plot\n",
        "if stats['total_predictions'] > 0:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Pie chart\n",
        "    if 'results' in locals():\n",
        "        tumor_counts = [sum(1 for r in results if r['has_tumor']),\n",
        "                        sum(1 for r in results if not r['has_tumor'])]\n",
        "        ax1.pie(tumor_counts, labels=['Tumor', 'No Tumor'], autopct='%1.1f%%',\n",
        "                colors=['#ff6b6b', '#51cf66'], startangle=90)\n",
        "        ax1.set_title('Tumor Detection Rate', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Bar chart\n",
        "    modality_data = [stats['mri_detections'], stats['pet_detections']]\n",
        "    ax2.bar(['MRI', 'PET'], modality_data, color=['#4dabf7', '#f06595'])\n",
        "    ax2.set_ylabel('Number of Predictions')\n",
        "    ax2.set_title('Predictions by Modality', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f24736a3",
      "metadata": {
        "id": "f24736a3"
      },
      "source": [
        "## 13. Model Performance Comparison\n",
        "\n",
        "Compare MRI-only vs PET-only vs Multi-modal approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3db88b",
      "metadata": {
        "id": "9d3db88b"
      },
      "outputs": [],
      "source": [
        "# Compare different approaches\n",
        "if os.path.exists(pet_model_path):\n",
        "    print(\"Comparing models...\")\n",
        "\n",
        "    # Create separate detectors\n",
        "    mri_only = MultiModalBrainTumorDetector(mri_model_path, None, False)\n",
        "    pet_only = MultiModalBrainTumorDetector(pet_model_path, None, False)\n",
        "    multimodal = detector\n",
        "\n",
        "    # Test on sample images\n",
        "    test_images_comp = list(Path(test_dir).glob(\"*.jpg\"))[:5]\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    for img in test_images_comp:\n",
        "        mri_res = mri_only.predict(str(img), modality='MRI')\n",
        "        pet_res = pet_only.predict(str(img), modality='PET')\n",
        "        mm_res = multimodal.predict(str(img))\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Image': os.path.basename(str(img)),\n",
        "            'MRI_Conf': max([d['confidence'] for d in mri_res['detections']], default=0),\n",
        "            'PET_Conf': max([d['confidence'] for d in pet_res['detections']], default=0),\n",
        "            'MM_Conf': max([d['confidence'] for d in mm_res['detections']], default=0),\n",
        "            'MM_Modality': mm_res['modality']\n",
        "        })\n",
        "\n",
        "    comp_df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n\" + comp_df.to_string(index=False))\n",
        "\n",
        "    # Plot comparison\n",
        "    comp_df[['MRI_Conf', 'PET_Conf', 'MM_Conf']].plot(kind='bar', figsize=(12, 6))\n",
        "    plt.title('Confidence Comparison: MRI vs PET vs Multi-Modal', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Test Image')\n",
        "    plt.ylabel('Confidence Score')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.legend(['MRI Only', 'PET Only', 'Multi-Modal'])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"PET model not trained yet. Train PET model to enable comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c42cb7",
      "metadata": {
        "id": "d4c42cb7"
      },
      "source": [
        "## 14. Summary and Next Steps\n",
        "\n",
        "### ✅ What We've Built:\n",
        "1. **DICOM Converter** - Convert medical images to YOLO format\n",
        "2. **Modality Classifier** - Automatically detect MRI vs PET\n",
        "3. **Multi-Modal Detector** - Integrate separate models\n",
        "4. **Visualization Tools** - Display results\n",
        "5. **Performance Comparison** - Compare approaches\n",
        "\n",
        "### 📋 Next Steps for Your Thesis:\n",
        "\n",
        "#### If PET Model Not Trained Yet:\n",
        "1. Annotate PET images using LabelImg/Roboflow\n",
        "2. Run training cell (Cell 7)\n",
        "3. Re-run comparison (Cell 13)\n",
        "\n",
        "#### For Thesis Analysis:\n",
        "1. **Collect metrics:** Precision, Recall, F1-score\n",
        "2. **Compare approaches:** Single vs Multi-modal\n",
        "3. **Analyze modality detection:** Accuracy of classification\n",
        "4. **Discuss results:** Clinical applicability\n",
        "\n",
        "### 🎓 Thesis Contribution:\n",
        "- Novel multi-modal integration approach\n",
        "- Automatic modality detection\n",
        "- Comparative analysis of detection methods\n",
        "- Potential for improved diagnostic accuracy\n",
        "\n",
        "### 📊 Suggested Thesis Sections:\n",
        "1. **Introduction** - Multi-modal medical imaging\n",
        "2. **Methodology** - YOLOv8, dual-model architecture\n",
        "3. **Implementation** - This notebook\n",
        "4. **Results** - Performance comparisons\n",
        "5. **Discussion** - Benefits and limitations\n",
        "6. **Conclusion** - Future work\n",
        "\n",
        "---\n",
        "\n",
        "**Questions or Issues?**\n",
        "- Check `MULTIMODAL_README.md` for detailed documentation\n",
        "- Review individual Python scripts for more features\n",
        "- Test with different confidence thresholds\n",
        "- Experiment with ensemble fusion methods"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}