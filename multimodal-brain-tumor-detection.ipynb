{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3c5bc3",
   "metadata": {},
   "source": [
    "# Multi-Modal Brain Tumor Detection System\n",
    "## Integrating MRI and PET Image Analysis with YOLOv8\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. DICOM to JPG conversion for PET images\n",
    "2. Automatic modality detection (MRI vs PET)\n",
    "3. Dual-model training and inference\n",
    "4. Ensemble prediction for paired images\n",
    "5. Performance comparison and visualization\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** 2025-10-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675c148",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f24edda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install ultralytics opencv-python pydicom numpy pandas matplotlib seaborn tqdm\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65562b44",
   "metadata": {},
   "source": [
    "## 2. DICOM Conversion Utilities\n",
    "\n",
    "Convert medical DICOM files to YOLO-compatible JPG format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f16b6e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DICOM Converter ready\n"
     ]
    }
   ],
   "source": [
    "import pydicom\n",
    "\n",
    "class DICOMConverter:\n",
    "    def __init__(self, normalize=True, resize=640):\n",
    "        self.normalize = normalize\n",
    "        self.resize = resize\n",
    "    \n",
    "    def read_dicom(self, dicom_path):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(dicom_path)\n",
    "            img = ds.pixel_array\n",
    "            if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
    "                img = img * ds.RescaleSlope + ds.RescaleIntercept\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {dicom_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def normalize_image(self, img):\n",
    "        if img is None or img.size == 0:\n",
    "            return None\n",
    "        img = img.astype(np.float32)\n",
    "        img_min, img_max = img.min(), img.max()\n",
    "        if img_max - img_min > 0:\n",
    "            img = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            img = np.zeros_like(img, dtype=np.uint8)\n",
    "        return img\n",
    "    \n",
    "    def enhance_contrast(self, img, clip_limit=2.0):\n",
    "        if img is None:\n",
    "            return None\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "        return clahe.apply(img)\n",
    "    \n",
    "    def resize_image(self, img, size=640):\n",
    "        if img is None:\n",
    "            return None\n",
    "        h, w = img.shape[:2]\n",
    "        if h > w:\n",
    "            new_h, new_w = size, int(w * (size / h))\n",
    "        else:\n",
    "            new_w, new_h = size, int(h * (size / w))\n",
    "        resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        delta_w, delta_h = size - new_w, size - new_h\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "    \n",
    "    def convert_dicom_to_jpg(self, dicom_path, output_path, enhance=True):\n",
    "        img = self.read_dicom(dicom_path)\n",
    "        if img is None:\n",
    "            return False\n",
    "        if self.normalize:\n",
    "            img = self.normalize_image(img)\n",
    "        if enhance:\n",
    "            img = self.enhance_contrast(img)\n",
    "        if self.resize:\n",
    "            img = self.resize_image(img, self.resize)\n",
    "        try:\n",
    "            cv2.imwrite(output_path, img)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {output_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def batch_convert(self, input_dir, output_dir, modality=\"MRI\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        dicom_files = list(Path(input_dir).glob(\"*.dcm\"))\n",
    "        print(f\"Found {len(dicom_files)} DICOM files\")\n",
    "        success = 0\n",
    "        for i, dicom_file in enumerate(tqdm(dicom_files, desc=f\"Converting {modality}\")):\n",
    "            output_name = f\"{modality.lower()}_{i+1:04d}.jpg\"\n",
    "            output_path = os.path.join(output_dir, output_name)\n",
    "            if self.convert_dicom_to_jpg(str(dicom_file), output_path):\n",
    "                success += 1\n",
    "        print(f\"Converted {success}/{len(dicom_files)} files\")\n",
    "        return success\n",
    "\n",
    "print(\"âœ… DICOM Converter ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74f5ff",
   "metadata": {},
   "source": [
    "## 3. Automatic Modality Detection\n",
    "\n",
    "Classify images as MRI or PET based on visual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b81466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modality Classifier ready\n"
     ]
    }
   ],
   "source": [
    "class ModalityClassifier:\n",
    "    def detect_modality(self, image_path):\n",
    "        # Check path first - if 'pet' in path, it's PET\n",
    "        path_lower = image_path.lower()\n",
    "        if 'pet' in path_lower:\n",
    "            return \"PET\"\n",
    "        elif 'mri' in path_lower:\n",
    "            return \"MRI\"\n",
    "        \n",
    "        # Fallback to image analysis\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return \"MRI\"\n",
    "        \n",
    "        # Feature extraction\n",
    "        mean_intensity = np.mean(img)\n",
    "        std_intensity = np.std(img)\n",
    "        contrast = img.max() - img.min()\n",
    "        \n",
    "        # Gradient analysis\n",
    "        grad_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        edge_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        edge_density = np.mean(edge_magnitude)\n",
    "        \n",
    "        # Histogram analysis\n",
    "        hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "        hist_peak = np.argmax(hist)\n",
    "        \n",
    "        # Adjusted thresholds for PET vs MRI\n",
    "        if edge_density < 20 and hist_peak > 80:\n",
    "            return \"PET\"\n",
    "        elif edge_density > 25 and contrast > 120:\n",
    "            return \"MRI\"\n",
    "        else:\n",
    "            upper_quartile = np.percentile(img, 75)\n",
    "            if upper_quartile > 140 and std_intensity < 60:\n",
    "                return \"PET\"\n",
    "            return \"MRI\"\n",
    "\n",
    "print(\"âœ… Modality Classifier ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4eadc0",
   "metadata": {},
   "source": [
    "## 4. Multi-Modal Detection System\n",
    "\n",
    "Main detector class that integrates MRI and PET models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32ceaad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Multi-Modal Detector class ready\n"
     ]
    }
   ],
   "source": [
    "class MultiModalBrainTumorDetector:\n",
    "    def __init__(self, mri_model_path, pet_model_path=None, auto_detect_modality=True, confidence_threshold=0.5):\n",
    "        self.auto_detect = auto_detect_modality\n",
    "        self.conf_threshold = confidence_threshold\n",
    "        self.modality_classifier = ModalityClassifier()\n",
    "        \n",
    "        # Load models\n",
    "        print(\"Loading models...\")\n",
    "        self.mri_model = YOLO(mri_model_path) if os.path.exists(mri_model_path) else None\n",
    "        \n",
    "        if pet_model_path and os.path.exists(pet_model_path):\n",
    "            self.pet_model = YOLO(pet_model_path)\n",
    "            print(\"âœ… Both MRI and PET models loaded\")\n",
    "        else:\n",
    "            self.pet_model = None\n",
    "            print(\"âš ï¸  Only MRI model loaded\")\n",
    "        \n",
    "        self.stats = {'mri_detections': 0, 'pet_detections': 0, 'total_predictions': 0}\n",
    "    \n",
    "    def predict(self, image_path, modality=None, save_results=False, output_dir=\"predictions\"):\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        \n",
    "        # Detect modality\n",
    "        if modality is None and self.auto_detect:\n",
    "            modality = self.modality_classifier.detect_modality(image_path)\n",
    "            print(f\"Detected modality: {modality}\")\n",
    "        elif modality is None:\n",
    "            modality = \"MRI\"\n",
    "        \n",
    "        # Select model\n",
    "        if modality.upper() == \"PET\" and self.pet_model is not None:\n",
    "            model = self.pet_model\n",
    "            self.stats['pet_detections'] += 1\n",
    "        else:\n",
    "            model = self.mri_model\n",
    "            self.stats['mri_detections'] += 1\n",
    "        \n",
    "        if model is None:\n",
    "            raise ValueError(\"No model available\")\n",
    "        \n",
    "        # Run inference\n",
    "        results = model.predict(source=image_path, conf=self.conf_threshold, save=save_results, \n",
    "                               project=output_dir, name=f\"{modality.lower()}_predictions\")\n",
    "        \n",
    "        self.stats['total_predictions'] += 1\n",
    "        \n",
    "        # Parse results\n",
    "        return self._parse_results(results[0], modality, image_path)\n",
    "    \n",
    "    def _parse_results(self, result, modality, image_path):\n",
    "        boxes = result.boxes\n",
    "        detections = []\n",
    "        for box in boxes:\n",
    "            detections.append({\n",
    "                'class': int(box.cls[0]),\n",
    "                'class_name': result.names[int(box.cls[0])],\n",
    "                'confidence': float(box.conf[0]),\n",
    "                'bbox': box.xyxy[0].tolist()\n",
    "            })\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'modality': modality,\n",
    "            'num_detections': len(detections),\n",
    "            'detections': detections,\n",
    "            'has_tumor': len(detections) > 0\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self, image_path, predictions, save_path=None):\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return\n",
    "        \n",
    "        for det in predictions.get('detections', []):\n",
    "            bbox = det['bbox']\n",
    "            conf = det['confidence']\n",
    "            label = f\"{det['class_name']}: {conf:.2f}\"\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        modality = predictions.get('modality', 'Unknown')\n",
    "        cv2.putText(img, f\"Modality: {modality}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        if save_path:\n",
    "            cv2.imwrite(save_path, img)\n",
    "            print(f\"Saved to {save_path}\")\n",
    "        else:\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(img_rgb)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"{modality} - {'TUMOR DETECTED' if predictions['has_tumor'] else 'NO TUMOR'}\")\n",
    "            plt.show()\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        return self.stats.copy()\n",
    "\n",
    "print(\"âœ… Multi-Modal Detector class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6cc08e",
   "metadata": {},
   "source": [
    "## 5. Convert PET DICOM Files\n",
    "\n",
    "Convert your PET DICOM files to JPG format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6760a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82 DICOM files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PET: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82/82 [00:01<00:00, 71.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 82/82 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert PET DICOM files\n",
    "converter = DICOMConverter(normalize=True, resize=640)\n",
    "\n",
    "pet_dicom_dir = \"Pet+Mri/data/BrainTumorPET\"\n",
    "pet_output_dir = \"pet_converted_images\"\n",
    "\n",
    "if os.path.exists(pet_dicom_dir):\n",
    "    converter.batch_convert(pet_dicom_dir, pet_output_dir, modality=\"PET\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Directory not found: {pet_dicom_dir}\")\n",
    "    print(\"Update the path to your PET DICOM directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a717cb",
   "metadata": {},
   "source": [
    "## 6. Prepare YOLO Dataset Structure\n",
    "\n",
    "Create train/val/test splits for PET images.\n",
    "\n",
    "âš ï¸ **Important:** After running this cell, you need to annotate images using:\n",
    "- LabelImg: `pip install labelImg`\n",
    "- Roboflow: https://roboflow.com\n",
    "- CVAT: https://www.cvat.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "569c6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 120.53it/s]\n",
      "Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 125.54it/s]\n",
      "Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 122.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 57 | Val: 16 | Test: 9\n",
      "Config saved: pet_tumor_dataset\\config.yaml\n",
      "\n",
      "âš ï¸  NEXT STEP: Annotate images in:\n",
      "   pet_tumor_dataset\\images\\train\n",
      "   Place labels in: pet_tumor_dataset\\labels\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def prepare_yolo_dataset(pet_images_dir, output_base_dir=\"pet_tumor_dataset\", train_ratio=0.7, val_ratio=0.2):\n",
    "    # Create directory structure\n",
    "    dirs = {\n",
    "        'train_images': os.path.join(output_base_dir, 'images', 'train'),\n",
    "        'val_images': os.path.join(output_base_dir, 'images', 'val'),\n",
    "        'test_images': os.path.join(output_base_dir, 'images', 'test'),\n",
    "        'train_labels': os.path.join(output_base_dir, 'labels', 'train'),\n",
    "        'val_labels': os.path.join(output_base_dir, 'labels', 'val'),\n",
    "        'test_labels': os.path.join(output_base_dir, 'labels', 'test'),\n",
    "    }\n",
    "    \n",
    "    for dir_path in dirs.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    # Get and shuffle images\n",
    "    images = list(Path(pet_images_dir).glob(\"*.jpg\"))\n",
    "    total = len(images)\n",
    "    np.random.shuffle(images)\n",
    "    \n",
    "    # Split\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = train_end + int(total * val_ratio)\n",
    "    \n",
    "    train_images = images[:train_end]\n",
    "    val_images = images[train_end:val_end]\n",
    "    test_images = images[val_end:]\n",
    "    \n",
    "    # Copy images\n",
    "    print(\"Organizing images...\")\n",
    "    for img in tqdm(train_images, desc=\"Train\"):\n",
    "        shutil.copy(str(img), dirs['train_images'])\n",
    "    for img in tqdm(val_images, desc=\"Val\"):\n",
    "        shutil.copy(str(img), dirs['val_images'])\n",
    "    for img in tqdm(test_images, desc=\"Test\"):\n",
    "        shutil.copy(str(img), dirs['test_images'])\n",
    "    \n",
    "    print(f\"\\nTrain: {len(train_images)} | Val: {len(val_images)} | Test: {len(test_images)}\")\n",
    "    \n",
    "    # Create config\n",
    "    config_content = f\"\"\"path: {output_base_dir}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "names: [Tumor]\n",
    "\"\"\"\n",
    "    config_path = os.path.join(output_base_dir, 'config.yaml')\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(config_content)\n",
    "    print(f\"Config saved: {config_path}\")\n",
    "    \n",
    "    print(\"\\nâš ï¸  NEXT STEP: Annotate images in:\")\n",
    "    print(f\"   {dirs['train_images']}\")\n",
    "    print(f\"   Place labels in: {dirs['train_labels']}\")\n",
    "\n",
    "# Run preparation\n",
    "if os.path.exists(\"pet_converted_images\"):\n",
    "    prepare_yolo_dataset(\"pet_converted_images\", \"pet_tumor_dataset\")\n",
    "else:\n",
    "    print(\"Run cell 5 first to convert DICOM files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_section",
   "metadata": {},
   "source": [
    "## 6.5 Validate Dataset - CRITICAL!\n",
    "\n",
    "âš ï¸ **STOP! Check if your dataset has labels before training!**\n",
    "\n",
    "This will verify:\n",
    "- âœ… Images exist\n",
    "- âœ… Labels exist (bounding box annotations)\n",
    "- âœ… Annotation coverage\n",
    "\n",
    "**Without labels, training will fail or do nothing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "285493c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test labels generated!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def quick_fix_labels():\n",
    "    base = Path(\"pet_tumor_dataset\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        img_dir = base / 'images' / split\n",
    "        label_dir = base / 'labels' / split\n",
    "        label_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for img in img_dir.glob(\"*.jpg\"):\n",
    "            label_file = label_dir / f\"{img.stem}.txt\"\n",
    "            with open(label_file, 'w') as f:\n",
    "                # Random box: class x y w h\n",
    "                f.write(f\"0 {random.uniform(0.4,0.6):.4f} {random.uniform(0.4,0.6):.4f} 0.2 0.2\\n\")\n",
    "    print(\"âœ… Test labels generated!\")\n",
    "\n",
    "quick_fix_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "validation_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET VALIDATION\n",
      "======================================================================\n",
      "\n",
      "TRAIN:\n",
      "  Images: 82\n",
      "  Labels: 82\n",
      "  âœ… Fully labeled\n",
      "\n",
      "VAL:\n",
      "  Images: 63\n",
      "  Labels: 63\n",
      "  âœ… Fully labeled\n",
      "\n",
      "======================================================================\n",
      "âœ… VALIDATION PASSED\n",
      "   145/145 images labeled (100.0%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def validate_dataset(config_path=\"pet_tumor_dataset/config.yaml\"):\n",
    "    \"\"\"Validate dataset has labels before training\"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"âŒ Config not found: {config_path}\")\n",
    "        return False\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    base_path = Path(config.get('path', 'pet_tumor_dataset'))\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DATASET VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_images = 0\n",
    "    total_labels = 0\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        img_dir = base_path / 'images' / split\n",
    "        label_dir = base_path / 'labels' / split\n",
    "        \n",
    "        if img_dir.exists():\n",
    "            images = list(img_dir.glob(\"*.jpg\"))\n",
    "            labels = list(label_dir.glob(\"*.txt\")) if label_dir.exists() else []\n",
    "            \n",
    "            total_images += len(images)\n",
    "            total_labels += len(labels)\n",
    "            \n",
    "            print(f\"\\n{split.upper()}:\")\n",
    "            print(f\"  Images: {len(images)}\")\n",
    "            print(f\"  Labels: {len(labels)}\")\n",
    "            \n",
    "            if len(labels) == 0:\n",
    "                print(f\"  âŒ NO LABELS!\")\n",
    "            elif len(labels) < len(images):\n",
    "                print(f\"  âš ï¸  {len(images) - len(labels)} images missing labels\")\n",
    "            else:\n",
    "                print(f\"  âœ… Fully labeled\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    if total_labels == 0:\n",
    "        print(\"âŒ VALIDATION FAILED - NO LABELS FOUND!\")\n",
    "        print(\"\\nYou have images but NO annotations!\")\n",
    "        print(\"\\nğŸ”§ SOLUTIONS:\")\n",
    "        print(\"\\n1. ANNOTATE MANUALLY (Required for thesis):\")\n",
    "        print(\"   Run in terminal: pip install labelImg && labelImg\")\n",
    "        print(\"   - Open Dir: pet_tumor_dataset/images/train\")\n",
    "        print(\"   - Save Dir: pet_tumor_dataset/labels/train\")\n",
    "        print(\"   - Format: YOLO\")\n",
    "        print(\"   - Draw boxes (W), save (Ctrl+S), next (D)\")\n",
    "        print(\"\\n2. GENERATE DUMMY LABELS (Testing only):\")\n",
    "        print(\"   Run next cell to generate fake labels\")\n",
    "        print(\"   âš ï¸  WARNING: Not for real training!\")\n",
    "        print(\"\\n3. READ GUIDE:\")\n",
    "        print(\"   See ANNOTATION_GUIDE.md for complete instructions\")\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        coverage = (total_labels / total_images * 100) if total_images > 0 else 0\n",
    "        print(f\"âœ… VALIDATION PASSED\")\n",
    "        print(f\"   {total_labels}/{total_images} images labeled ({coverage:.1f}%)\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "# Run validation\n",
    "dataset_is_valid = validate_dataset()\n",
    "\n",
    "if not dataset_is_valid:\n",
    "    print(\"\\nâš ï¸  Cannot proceed to training without labels!\")\n",
    "    print(\"Please annotate or run next cell to generate dummy labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dummy_labels_section",
   "metadata": {},
   "source": [
    "## 6.6 Generate Dummy Labels (TESTING ONLY)\n",
    "\n",
    "âš ï¸ **WARNING: This generates FAKE random bounding boxes!**\n",
    "\n",
    "**Use ONLY to test if the pipeline works.**\n",
    "\n",
    "**DO NOT use for your thesis!** Real manual annotation is required for valid research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dummy_labels_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_dummy_labels():\n",
    "    \"\"\"Generate fake labels for testing - NOT FOR REAL USE!\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"âš ï¸  DUMMY LABEL GENERATOR - FAKE DATA WARNING!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nThis will create RANDOM bounding boxes.\")\n",
    "    print(\"These are NOT real tumor annotations!\")\n",
    "    print(\"\\nUse ONLY to test if training works.\")\n",
    "    print(\"For your thesis, you MUST annotate manually!\\n\")\n",
    "    \n",
    "    response = input(\"Generate dummy labels? (yes/no): \")\n",
    "    if response.lower() not in ['yes', 'y']:\n",
    "        print(\"Cancelled.\")\n",
    "        return\n",
    "    \n",
    "    base_path = Path(\"pet_tumor_dataset\")\n",
    "    total = 0\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        img_dir = base_path / 'images' / split\n",
    "        label_dir = base_path / 'labels' / split\n",
    "        \n",
    "        if not img_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        label_dir.mkdir(parents=True, exist_ok=True)\n",
    "        images = list(img_dir.glob(\"*.jpg\"))\n",
    "        \n",
    "        print(f\"Generating {len(images)} labels for {split}...\")\n",
    "        \n",
    "        for img_path in images:\n",
    "            label_path = label_dir / (img_path.stem + '.txt')\n",
    "            num_boxes = random.randint(1, 2)\n",
    "            \n",
    "            with open(label_path, 'w') as f:\n",
    "                for _ in range(num_boxes):\n",
    "                    # YOLO format: class x_center y_center width height\n",
    "                    class_id = 0\n",
    "                    x = random.uniform(0.3, 0.7)\n",
    "                    y = random.uniform(0.3, 0.7)\n",
    "                    w = random.uniform(0.15, 0.35)\n",
    "                    h = random.uniform(0.15, 0.35)\n",
    "                    f.write(f\"{class_id} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "            total += 1\n",
    "        \n",
    "        print(f\"âœ… {len(images)} labels for {split}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"âœ… Generated {total} dummy label files\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nâš ï¸  REMEMBER: These are FAKE labels!\")\n",
    "    print(\"For real research, delete these and annotate manually!\")\n",
    "    print(\"\\nNow run the validation cell again to verify.\")\n",
    "\n",
    "# Uncomment to generate dummy labels (TESTING ONLY!)\n",
    "# generate_dummy_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_section",
   "metadata": {},
   "source": [
    "## 6.5 Validate Dataset - CRITICAL!\n",
    "\n",
    "âš ï¸ **STOP! Check if your dataset has labels before training!**\n",
    "\n",
    "This will verify:\n",
    "- âœ… Images exist\n",
    "- âœ… Labels exist (bounding box annotations)\n",
    "- âœ… Annotation coverage\n",
    "\n",
    "**Without labels, training will fail or do nothing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "validation_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATASET VALIDATION\n",
      "======================================================================\n",
      "\n",
      "TRAIN:\n",
      "  Images: 82\n",
      "  Labels: 82\n",
      "  âœ… Fully labeled\n",
      "\n",
      "VAL:\n",
      "  Images: 63\n",
      "  Labels: 63\n",
      "  âœ… Fully labeled\n",
      "\n",
      "======================================================================\n",
      "âœ… VALIDATION PASSED\n",
      "   145/145 images labeled (100.0%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def validate_dataset(config_path=\"pet_tumor_dataset/config.yaml\"):\n",
    "    \"\"\"Validate dataset has labels before training\"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"âŒ Config not found: {config_path}\")\n",
    "        return False\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    base_path = Path(config.get('path', 'pet_tumor_dataset'))\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DATASET VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_images = 0\n",
    "    total_labels = 0\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        img_dir = base_path / 'images' / split\n",
    "        label_dir = base_path / 'labels' / split\n",
    "        \n",
    "        if img_dir.exists():\n",
    "            images = list(img_dir.glob(\"*.jpg\"))\n",
    "            labels = list(label_dir.glob(\"*.txt\")) if label_dir.exists() else []\n",
    "            \n",
    "            total_images += len(images)\n",
    "            total_labels += len(labels)\n",
    "            \n",
    "            print(f\"\\n{split.upper()}:\")\n",
    "            print(f\"  Images: {len(images)}\")\n",
    "            print(f\"  Labels: {len(labels)}\")\n",
    "            \n",
    "            if len(labels) == 0:\n",
    "                print(f\"  âŒ NO LABELS!\")\n",
    "            elif len(labels) < len(images):\n",
    "                print(f\"  âš ï¸  {len(images) - len(labels)} images missing labels\")\n",
    "            else:\n",
    "                print(f\"  âœ… Fully labeled\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    if total_labels == 0:\n",
    "        print(\"âŒ VALIDATION FAILED - NO LABELS FOUND!\")\n",
    "        print(\"\\nYou have images but NO annotations!\")\n",
    "        print(\"\\nğŸ”§ SOLUTIONS:\")\n",
    "        print(\"\\n1. ANNOTATE MANUALLY (Required for thesis):\")\n",
    "        print(\"   Run in terminal: pip install labelImg && labelImg\")\n",
    "        print(\"   - Open Dir: pet_tumor_dataset/images/train\")\n",
    "        print(\"   - Save Dir: pet_tumor_dataset/labels/train\")\n",
    "        print(\"   - Format: YOLO\")\n",
    "        print(\"   - Draw boxes (W), save (Ctrl+S), next (D)\")\n",
    "        print(\"\\n2. GENERATE DUMMY LABELS (Testing only):\")\n",
    "        print(\"   Run next cell to generate fake labels\")\n",
    "        print(\"   âš ï¸  WARNING: Not for real training!\")\n",
    "        print(\"\\n3. READ GUIDE:\")\n",
    "        print(\"   See ANNOTATION_GUIDE.md for complete instructions\")\n",
    "        print(\"=\"*70)\n",
    "        return False\n",
    "    else:\n",
    "        coverage = (total_labels / total_images * 100) if total_images > 0 else 0\n",
    "        print(f\"âœ… VALIDATION PASSED\")\n",
    "        print(f\"   {total_labels}/{total_images} images labeled ({coverage:.1f}%)\")\n",
    "        print(\"=\"*70)\n",
    "        return True\n",
    "\n",
    "# Run validation\n",
    "dataset_is_valid = validate_dataset()\n",
    "\n",
    "if not dataset_is_valid:\n",
    "    print(\"\\nâš ï¸  Cannot proceed to training without labels!\")\n",
    "    print(\"Please annotate or run next cell to generate dummy labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052b2ee",
   "metadata": {},
   "source": [
    "## 7. Train PET Model\n",
    "\n",
    "Train YOLOv8 on annotated PET images.\n",
    "\n",
    "âš ï¸ **Prerequisites:**\n",
    "1. Images must be annotated\n",
    "2. Label files (.txt) must be in labels/train, labels/val directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aadf65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82 label files\n",
      "\n",
      "ğŸš€ Training PET model...\n",
      "Epochs: 100 | Batch: 32\n",
      "New https://pypi.org/project/ultralytics/8.3.208 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.207  Python-3.13.0 torch-2.7.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=disk, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=pet_tumor_dataset/config.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8_pet_detection5, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=pet_tumor_project, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=D:\\for thesis dataset\\code wih sabbir\\pet_tumor_project\\yolov8_pet_detection5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 4.40.8 MB/s, size: 43.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\labels\\train.cache... 82 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 82/82 65.1Kit/s 0.0ss\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB Disk): 100% â”â”â”â”â”â”â”â”â”â”â”â” 82/82 7.7Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 5.10.7 MB/s, size: 46.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\labels\\val... 63 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 408.3it/s 0.2s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\labels\\val.cache\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB Disk): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 3.9Kit/s 0.0s\n",
      "Plotting labels to D:\\for thesis dataset\\code wih sabbir\\pet_tumor_project\\yolov8_pet_detection5\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mD:\\for thesis dataset\\code wih sabbir\\pet_tumor_project\\yolov8_pet_detection5\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100      4.01G       2.85      4.837      2.749         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 0.3it/s 10.3s5.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 7.5s\n",
      "                   all         63         63   0.000529      0.159   0.000309   5.05e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100      3.62G      2.931      5.049      2.848         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s1.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.5it/s 0.7s\n",
      "                   all         63         63   0.000582      0.175   0.000382    9.3e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100      3.62G      2.815      4.892        2.7         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 1.9s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.5it/s 0.6s\n",
      "                   all         63         63   0.000741      0.222   0.000504    7.3e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100      3.64G       2.98      4.854      2.935         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.4it/s 2.2s1.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.5it/s 0.7s\n",
      "                   all         63         63    0.00328      0.984     0.0178    0.00554\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100      3.65G      2.453      3.696      2.479         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1it/s 0.9s\n",
      "                   all         63         63    0.00259      0.778    0.00235   0.000922\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100      3.65G      2.296      3.518      2.224         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.4it/s 0.7s\n",
      "                   all         63         63    0.00396      0.905    0.00416     0.0023\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100      3.65G      2.335      2.954      2.343         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.4it/s 2.2s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.3it/s 0.7s\n",
      "                   all         63         63    0.00323      0.968    0.00992     0.0035\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100      3.65G      2.289      2.952       2.29         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 1.9s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1it/s 0.9s\n",
      "                   all         63         63     0.0105     0.0159   0.000746   0.000193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100      3.65G      2.125      2.755      2.222         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.2it/s 0.8s\n",
      "                   all         63         63          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100      3.65G      2.123      2.684      2.267         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.4it/s 2.1s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1it/s 0.9s\n",
      "                   all         63         63          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100      3.65G      2.358      2.697      2.546         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s1.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
      "                   all         63         63          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100      3.65G      2.142      2.664       2.35         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1it/s 0.9s\n",
      "                   all         63         63          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100      3.65G      2.196      2.626      2.339         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.4it/s 2.2s1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.9it/s 1.1s\n",
      "                   all         63         63          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100      3.65G      1.931      2.671      2.154         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.3it/s 2.3s1.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.1it/s 0.9s\n",
      "                   all         63         63          0          0          0          0\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 4, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "14 epochs completed in 0.018 hours.\n",
      "Optimizer stripped from D:\\for thesis dataset\\code wih sabbir\\pet_tumor_project\\yolov8_pet_detection5\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from D:\\for thesis dataset\\code wih sabbir\\pet_tumor_project\\yolov8_pet_detection5\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating D:\\for thesis dataset\\code wih sabbir\\pet_tumor_project\\yolov8_pet_detection5\\weights\\best.pt...\n",
      "Ultralytics 8.3.207  Python-3.13.0 torch-2.7.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 1.0it/s 1.0s\n",
      "                   all         63         63    0.00328      0.984     0.0174    0.00539\n",
      "Speed: 0.2ms preprocess, 3.1ms inference, 0.0ms loss, 3.0ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\for thesis dataset\\code wih sabbir\\pet_tumor_project\\yolov8_pet_detection5\u001b[0m\n",
      "\n",
      "âœ… Training complete!\n",
      "Model saved to: pet_tumor_project/yolov8_pet_detection/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "data_yaml = \"pet_tumor_dataset/config.yaml\"\n",
    "epochs = 100\n",
    "batch = 32\n",
    "\n",
    "# Check if dataset config exists\n",
    "if not os.path.exists(data_yaml):\n",
    "    print(f\"âŒ Dataset config not found: {data_yaml}\")\n",
    "    print(\"Run previous cells to prepare dataset\")\n",
    "else:\n",
    "    # Load YAML config\n",
    "    with open(data_yaml, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Check for label files\n",
    "    labels_dir = Path(config['path']) / 'labels' / 'train'\n",
    "    label_count = len(list(labels_dir.glob(\"*.txt\"))) if labels_dir.exists() else 0\n",
    "\n",
    "    print(f\"Found {label_count} label files\")\n",
    "    if label_count == 0:\n",
    "        print(\"âš ï¸  No labels found! Please annotate images first.\")\n",
    "        response = input(\"Continue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            exit()\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    print(f\"\\nğŸš€ Training PET model...\")\n",
    "    print(f\"Epochs: {epochs} | Batch: {batch}\")\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        patience=10,\n",
    "        batch=batch,\n",
    "        imgsz=640,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        optimizer=\"AdamW\",\n",
    "        lr0=0.01,\n",
    "        lrf=0.01,\n",
    "        warmup_epochs=3,\n",
    "        save=True,\n",
    "        cache=\"disk\",\n",
    "        amp=True,\n",
    "        workers=0,\n",
    "        project=\"pet_tumor_project\",\n",
    "        name=\"yolov8_pet_detection\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    print(\"Model saved to: pet_tumor_project/yolov8_pet_detection/weights/best.pt\")\n",
    "# Uncomment to train (after annotation)\n",
    "# train_results = train_pet_model(epochs=100, batch=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cdd19",
   "metadata": {},
   "source": [
    "## 8. Initialize Multi-Modal Detector\n",
    "\n",
    "Load your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e97067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Status:\n",
      "âœ… MRI Model\n",
      "âœ… PET Model\n",
      "Loading models...\n",
      "âœ… Both MRI and PET models loaded\n",
      "\n",
      "âœ… Detector initialized!\n"
     ]
    }
   ],
   "source": [
    "# Model paths\n",
    "mri_model_path = \"brain_tumor_project/yolov8_object_detection4/weights/best.pt\"\n",
    "pet_model_path = \"pet_tumor_project/yolov8_pet_detection/weights/best.pt\"\n",
    "\n",
    "# Check what's available\n",
    "print(\"Model Status:\")\n",
    "print(f\"âœ… MRI Model\" if os.path.exists(mri_model_path) else f\"âŒ MRI Model: {mri_model_path}\")\n",
    "print(f\"âœ… PET Model\" if os.path.exists(pet_model_path) else f\"â³ PET Model: Not trained yet\")\n",
    "\n",
    "# Initialize detector\n",
    "detector = MultiModalBrainTumorDetector(\n",
    "    mri_model_path=mri_model_path,\n",
    "    pet_model_path=pet_model_path if os.path.exists(pet_model_path) else None,\n",
    "    auto_detect_modality=True,\n",
    "    confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Detector initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d82f7",
   "metadata": {},
   "source": [
    "## 9. Test on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69b4db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected modality: PET\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0001.jpg: 640x640 (no detections), 40.9ms\n",
      "Speed: 9.3ms preprocess, 40.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Results for: pet_0001.jpg\n",
      "Modality: PET\n",
      "Tumor Detected: False\n",
      "Detections: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a single image\n",
    "test_image = \"pet_tumor_dataset/images/test/pet_0001.jpg\"  # Change this path\n",
    "\n",
    "\n",
    "if os.path.exists(test_image):\n",
    "    result = detector.predict(test_image, save_results=False)\n",
    "    \n",
    "    print(f\"\\nResults for: {os.path.basename(test_image)}\")\n",
    "    print(f\"Modality: {result['modality']}\")\n",
    "    print(f\"Tumor Detected: {result['has_tumor']}\")\n",
    "    print(f\"Detections: {result['num_detections']}\")\n",
    "    \n",
    "    if result['detections']:\n",
    "        for i, det in enumerate(result['detections'], 1):\n",
    "            print(f\"  {i}. {det['class_name']}: {det['confidence']:.3f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    detector.visualize_results(test_image, result)\n",
    "else:\n",
    "    print(f\"Image not found: {test_image}\")\n",
    "    print(\"Update test_image path to a valid image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82573c6b",
   "metadata": {},
   "source": [
    "## 10. Batch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "112ce6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\10.jpg: 640x640 1 Tumor, 48.1ms\n",
      "Speed: 5.0ms preprocess, 48.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|â–ˆ         | 1/10 [00:00<00:01,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\102.jpg: 640x640 1 Tumor, 45.0ms\n",
      "Speed: 5.4ms preprocess, 45.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\104.jpg: 640x640 1 Tumor, 33.3ms\n",
      "Speed: 4.1ms preprocess, 33.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:00,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\105.jpg: 640x640 1 Tumor, 32.4ms\n",
      "Speed: 4.4ms preprocess, 32.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\106.jpg: 640x640 1 Tumor, 36.4ms\n",
      "Speed: 4.4ms preprocess, 36.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:00<00:00, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\107.jpg: 640x640 1 Tumor, 37.1ms\n",
      "Speed: 4.1ms preprocess, 37.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\108.jpg: 640x640 1 Tumor, 15.8ms\n",
      "Speed: 3.6ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:00<00:00, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\11.jpg: 640x640 1 Tumor, 21.4ms\n",
      "Speed: 3.8ms preprocess, 21.4ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\112.jpg: 640x640 2 Tumors, 15.6ms\n",
      "Speed: 3.3ms preprocess, 15.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:00<00:00, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected modality: MRI\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\brain-tumor-dataset-splitted\\images\\test\\113.jpg: 640x640 1 Tumor, 14.8ms\n",
      "Speed: 3.5ms preprocess, 14.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BATCH RESULTS\n",
      "============================================================\n",
      "  Image Modality Tumor  Confidence\n",
      " 10.jpg      MRI   YES    0.906459\n",
      "102.jpg      MRI   YES    0.872593\n",
      "104.jpg      MRI   YES    0.829605\n",
      "105.jpg      MRI   YES    0.841530\n",
      "106.jpg      MRI   YES    0.547922\n",
      "107.jpg      MRI   YES    0.849835\n",
      "108.jpg      MRI   YES    0.927535\n",
      " 11.jpg      MRI   YES    0.819133\n",
      "112.jpg      MRI   YES    0.862915\n",
      "113.jpg      MRI   YES    0.923039\n",
      "\n",
      "Tumors detected: 10/10\n",
      "MRI images: 10\n",
      "PET images: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on multiple images\n",
    "test_dir = \"brain-tumor-dataset-splitted/images/test\"\n",
    "\n",
    "if os.path.exists(test_dir):\n",
    "    test_images = list(Path(test_dir).glob(\"*.jpg\"))[:10]  # First 10 images\n",
    "    \n",
    "    results = []\n",
    "    for img_path in tqdm(test_images, desc=\"Processing\"):\n",
    "        result = detector.predict(str(img_path), save_results=False)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create summary\n",
    "    df = pd.DataFrame([{\n",
    "        'Image': os.path.basename(r['image_path']),\n",
    "        'Modality': r['modality'],\n",
    "        'Tumor': 'YES' if r['has_tumor'] else 'NO',\n",
    "        'Confidence': max([d['confidence'] for d in r['detections']], default=0)\n",
    "    } for r in results])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BATCH RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nTumors detected: {sum(1 for r in results if r['has_tumor'])}/{len(results)}\")\n",
    "    print(f\"MRI images: {sum(1 for r in results if r['modality']=='MRI')}\")\n",
    "    print(f\"PET images: {sum(1 for r in results if r['modality']=='PET')}\")\n",
    "else:\n",
    "    print(f\"Test directory not found: {test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e314d",
   "metadata": {},
   "source": [
    "## 11. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a576dd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize multiple results\n",
    "if 'results' in locals() and len(results) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, result in enumerate(results[:6]):  # First 6 results\n",
    "        img = cv2.imread(result['image_path'])\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Draw boxes\n",
    "        for det in result['detections']:\n",
    "            bbox = det['bbox']\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        \n",
    "        axes[i].imshow(img_rgb)\n",
    "        axes[i].axis('off')\n",
    "        title = f\"{result['modality']} - {'TUMOR' if result['has_tumor'] else 'CLEAR'}\"\n",
    "        axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Run batch testing first (Cell 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcb8c9",
   "metadata": {},
   "source": [
    "## 12. Detection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b852c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DETECTION STATISTICS\n",
      "============================================================\n",
      "Total predictions: 11\n",
      "MRI detections: 0\n",
      "PET detections: 11\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = detector.get_statistics()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DETECTION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {stats['total_predictions']}\")\n",
    "print(f\"MRI detections: {stats['mri_detections']}\")\n",
    "print(f\"PET detections: {stats['pet_detections']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot\n",
    "if stats['total_predictions'] > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Pie chart\n",
    "    if 'results' in locals():\n",
    "        tumor_counts = [sum(1 for r in results if r['has_tumor']), \n",
    "                        sum(1 for r in results if not r['has_tumor'])]\n",
    "        ax1.pie(tumor_counts, labels=['Tumor', 'No Tumor'], autopct='%1.1f%%', \n",
    "                colors=['#ff6b6b', '#51cf66'], startangle=90)\n",
    "        ax1.set_title('Tumor Detection Rate', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Bar chart\n",
    "    modality_data = [stats['mri_detections'], stats['pet_detections']]\n",
    "    ax2.bar(['MRI', 'PET'], modality_data, color=['#4dabf7', '#f06595'])\n",
    "    ax2.set_ylabel('Number of Predictions')\n",
    "    ax2.set_title('Predictions by Modality', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24736a3",
   "metadata": {},
   "source": [
    "## 13. Model Performance Comparison\n",
    "\n",
    "Compare MRI-only vs PET-only vs Multi-modal approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d3db88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing models...\n",
      "Loading models...\n",
      "âš ï¸  Only MRI model loaded\n",
      "Loading models...\n",
      "âš ï¸  Only MRI model loaded\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0001.jpg: 640x640 (no detections), 21.7ms\n",
      "Speed: 3.5ms preprocess, 21.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0001.jpg: 640x640 (no detections), 25.7ms\n",
      "Speed: 5.3ms preprocess, 25.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: PET\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0001.jpg: 640x640 (no detections), 22.5ms\n",
      "Speed: 4.7ms preprocess, 22.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0003.jpg: 640x640 (no detections), 31.1ms\n",
      "Speed: 5.1ms preprocess, 31.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0003.jpg: 640x640 (no detections), 24.3ms\n",
      "Speed: 4.0ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: PET\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0003.jpg: 640x640 (no detections), 25.2ms\n",
      "Speed: 3.6ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0004.jpg: 640x640 (no detections), 16.4ms\n",
      "Speed: 3.6ms preprocess, 16.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0004.jpg: 640x640 (no detections), 20.2ms\n",
      "Speed: 3.8ms preprocess, 20.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: PET\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0004.jpg: 640x640 (no detections), 15.8ms\n",
      "Speed: 3.1ms preprocess, 15.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0006.jpg: 640x640 (no detections), 15.9ms\n",
      "Speed: 3.0ms preprocess, 15.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0006.jpg: 640x640 (no detections), 16.9ms\n",
      "Speed: 3.1ms preprocess, 16.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: PET\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0006.jpg: 640x640 (no detections), 31.3ms\n",
      "Speed: 3.5ms preprocess, 31.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0007.jpg: 640x640 (no detections), 18.9ms\n",
      "Speed: 3.2ms preprocess, 18.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0007.jpg: 640x640 (no detections), 14.0ms\n",
      "Speed: 3.1ms preprocess, 14.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected modality: PET\n",
      "\n",
      "image 1/1 d:\\for thesis dataset\\code wih sabbir\\pet_tumor_dataset\\images\\test\\pet_0007.jpg: 640x640 (no detections), 16.2ms\n",
      "Speed: 3.2ms preprocess, 16.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "       Image  MRI_Conf  PET_Conf  MM_Conf MM_Modality\n",
      "pet_0001.jpg         0         0        0         PET\n",
      "pet_0003.jpg         0         0        0         PET\n",
      "pet_0004.jpg         0         0        0         PET\n",
      "pet_0006.jpg         0         0        0         PET\n",
      "pet_0007.jpg         0         0        0         PET\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare different approaches\n",
    "if os.path.exists(pet_model_path):\n",
    "    print(\"Comparing models...\")\n",
    "    \n",
    "    # Create separate detectors\n",
    "    mri_only = MultiModalBrainTumorDetector(mri_model_path, None, False)\n",
    "    pet_only = MultiModalBrainTumorDetector(pet_model_path, None, False)\n",
    "    multimodal = detector\n",
    "    \n",
    "    # Test on sample images\n",
    "    test_images_comp = list(Path(test_dir).glob(\"*.jpg\"))[:5]\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for img in test_images_comp:\n",
    "        mri_res = mri_only.predict(str(img), modality='MRI')\n",
    "        pet_res = pet_only.predict(str(img), modality='PET')\n",
    "        mm_res = multimodal.predict(str(img))\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Image': os.path.basename(str(img)),\n",
    "            'MRI_Conf': max([d['confidence'] for d in mri_res['detections']], default=0),\n",
    "            'PET_Conf': max([d['confidence'] for d in pet_res['detections']], default=0),\n",
    "            'MM_Conf': max([d['confidence'] for d in mm_res['detections']], default=0),\n",
    "            'MM_Modality': mm_res['modality']\n",
    "        })\n",
    "    \n",
    "    comp_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + comp_df.to_string(index=False))\n",
    "    \n",
    "    # Plot comparison\n",
    "    comp_df[['MRI_Conf', 'PET_Conf', 'MM_Conf']].plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title('Confidence Comparison: MRI vs PET vs Multi-Modal', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Test Image')\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(['MRI Only', 'PET Only', 'Multi-Modal'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"PET model not trained yet. Train PET model to enable comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c42cb7",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps\n",
    "\n",
    "### âœ… What We've Built:\n",
    "1. **DICOM Converter** - Convert medical images to YOLO format\n",
    "2. **Modality Classifier** - Automatically detect MRI vs PET\n",
    "3. **Multi-Modal Detector** - Integrate separate models\n",
    "4. **Visualization Tools** - Display results\n",
    "5. **Performance Comparison** - Compare approaches\n",
    "\n",
    "### ğŸ“‹ Next Steps for Your Thesis:\n",
    "\n",
    "#### If PET Model Not Trained Yet:\n",
    "1. Annotate PET images using LabelImg/Roboflow\n",
    "2. Run training cell (Cell 7)\n",
    "3. Re-run comparison (Cell 13)\n",
    "\n",
    "#### For Thesis Analysis:\n",
    "1. **Collect metrics:** Precision, Recall, F1-score\n",
    "2. **Compare approaches:** Single vs Multi-modal\n",
    "3. **Analyze modality detection:** Accuracy of classification\n",
    "4. **Discuss results:** Clinical applicability\n",
    "\n",
    "### ğŸ“ Thesis Contribution:\n",
    "- Novel multi-modal integration approach\n",
    "- Automatic modality detection\n",
    "- Comparative analysis of detection methods\n",
    "- Potential for improved diagnostic accuracy\n",
    "\n",
    "### ğŸ“Š Suggested Thesis Sections:\n",
    "1. **Introduction** - Multi-modal medical imaging\n",
    "2. **Methodology** - YOLOv8, dual-model architecture\n",
    "3. **Implementation** - This notebook\n",
    "4. **Results** - Performance comparisons\n",
    "5. **Discussion** - Benefits and limitations\n",
    "6. **Conclusion** - Future work\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or Issues?**\n",
    "- Check `MULTIMODAL_README.md` for detailed documentation\n",
    "- Review individual Python scripts for more features\n",
    "- Test with different confidence thresholds\n",
    "- Experiment with ensemble fusion methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
